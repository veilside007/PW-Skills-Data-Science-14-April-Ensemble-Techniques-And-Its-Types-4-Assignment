{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d86bb-e5c8-4685-91ed-d2be02c364f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved.\n",
    "\n",
    "Dataset link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?\n",
    "usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cd6fb-1ba2-4e2c-bacd-fc3016a892e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "\n",
    "To build a random forest classifier to predict the risk of heart disease based on the provided dataset, we need to preprocess the dataset first. This preprocessing involves handling missing values, encoding categorical variables, and scaling numerical features if necessary. Let's go through each step:\n",
    "\n",
    "1. **Load the Dataset:**\n",
    "   - Download the dataset from the provided link.\n",
    "   - Read the dataset into a pandas DataFrame.\n",
    "\n",
    "2. **Handle Missing Values:**\n",
    "   - Check for any missing values in the dataset.\n",
    "   - Handle missing values by either imputing them with a suitable value (e.g., mean, median) or removing rows or columns containing missing values.\n",
    "\n",
    "3. **Encode Categorical Variables:**\n",
    "   - Identify categorical variables in the dataset (if any).\n",
    "   - Encode categorical variables using one-hot encoding or label encoding, depending on the nature of the categorical variables.\n",
    "\n",
    "4. **Scale Numerical Features:**\n",
    "   - If numerical features are on different scales, consider scaling them to ensure that they have similar ranges.\n",
    "   - Common scaling techniques include Min-Max scaling or Standardization.\n",
    "\n",
    "Let's implement these preprocessing steps in Python:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "heart_disease_data = pd.read_csv(url)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = heart_disease_data.drop(columns=['target'])\n",
    "y = heart_disease_data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "This code performs the following steps:\n",
    "- Loads the dataset.\n",
    "- Splits the dataset into features (X) and target variable (y).\n",
    "- Splits the data into training and testing sets.\n",
    "- Defines preprocessing steps using pipelines and transformers.\n",
    "- Creates a preprocessing pipeline that handles missing values, encodes categorical variables, and scales numerical features.\n",
    "- Builds a random forest classifier pipeline.\n",
    "- Fits the model on the training data.\n",
    "- Evaluates the model's accuracy on the testing data.\n",
    "\n",
    "This code provides a basic example of preprocessing and building a random forest classifier for the heart disease prediction task. You may need to adjust the preprocessing steps or model hyperparameters based on the specific characteristics of the dataset and the requirements of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34599be9-5ded-474e-a54c-5be7902fc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "\n",
    "To split the dataset into a training set and a test set using Python, we can use the `train_test_split` function from the `sklearn.model_selection` module. Here's how you can do it:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "heart_disease_data = pd.read_csv(url)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = heart_disease_data.drop(columns=['target'])\n",
    "y = heart_disease_data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "This code performs the following steps:\n",
    "1. Loads the dataset from the provided URL.\n",
    "2. Separates the features (X) and the target variable (y).\n",
    "3. Uses the `train_test_split` function to split the data into training and testing sets, with a test size of 30% and a random state of 42.\n",
    "4. Prints the shapes of the training and testing sets to verify the split.\n",
    "\n",
    "Adjust the `test_size` parameter in the `train_test_split` function call if you want to change the proportion of the test set. In this example, 70% of the data is used for training, and 30% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a1803-14b4-4bc1-a6a9-799d5a65aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "\n",
    "To train a Random Forest Classifier on the training set using 100 trees and a maximum depth of 10 for each tree, you can use the `RandomForestClassifier` class from the `sklearn.ensemble` module. Here's how you can do it:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "This code creates a Random Forest Classifier with 100 trees (`n_estimators=100`) and a maximum depth of 10 for each tree (`max_depth=10`). The `random_state` parameter is set to 42 for reproducibility.\n",
    "\n",
    "Then, the classifier is trained on the training set using the `fit` method, with the features (`X_train`) and the target variable (`y_train`).\n",
    "\n",
    "The default values for other hyperparameters will be used, such as the criterion for splitting (`gini`), the minimum samples required to split an internal node (`min_samples_split=2`), the minimum samples required to be at a leaf node (`min_samples_leaf=1`), etc.\n",
    "\n",
    "After training the classifier, you can use it to make predictions on new data or evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6f6cc-33c0-4c14-b8f0-0f6561225970",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "To evaluate the performance of the trained Random Forest Classifier on the test set, we can calculate various performance metrics such as accuracy, precision, recall, and F1 score. We can use the `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` functions from the `sklearn.metrics` module to compute these metrics. Here's how you can do it:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "This code:\n",
    "- Makes predictions on the test set using the trained Random Forest Classifier (`rf_classifier.predict(X_test)`).\n",
    "- Calculates accuracy, precision, recall, and F1 score using the ground truth labels (`y_test`) and the predicted labels (`y_pred`).\n",
    "- Prints out the computed performance metrics.\n",
    "\n",
    "These performance metrics provide insights into how well the model is performing on the test set. Accuracy measures the overall correctness of the predictions, precision measures the proportion of true positive predictions among all positive predictions, recall measures the proportion of true positive predictions among all actual positive instances, and F1 score is the harmonic mean of precision and recall, providing a balanced measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993837a-f454-48b6-adde-e312417042d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "To identify the top 5 most important features in predicting heart disease risk using the feature importance scores from the trained Random Forest Classifier, we can access the `feature_importances_` attribute of the classifier. Then, we can visualize the feature importances using a bar chart. Here's how you can do it:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Get the indices of the top 5 most important features\n",
    "indices = (-importances).argsort()[:5]\n",
    "\n",
    "# Get the names of the top 5 most important features\n",
    "top_features = X.columns[indices]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "plt.xticks(range(len(indices)), top_features, rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 5 Most Important Features in Predicting Heart Disease Risk')\n",
    "plt.show()\n",
    "\n",
    "This code:\n",
    "- Retrieves the feature importances from the trained Random Forest Classifier using the `feature_importances_` attribute.\n",
    "- Identifies the indices of the top 5 most important features by sorting the feature importances in descending order.\n",
    "- Gets the names of the top 5 most important features.\n",
    "- Plots the feature importances using a bar chart, with the names of the top features on the x-axis and their importance scores on the y-axis.\n",
    "\n",
    "This bar chart visualization helps identify which features are most influential in predicting heart disease risk according to the trained Random Forest Classifier. Features with higher importance scores contribute more to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b8618-4981-4d8d-a054-94f26e1302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try\n",
    "different values of the number of trees, maximum depth, minimum samples split, and minimum samples\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.\n",
    "\n",
    "To tune the hyperparameters of the Random Forest Classifier using either grid search or random search with cross-validation, we can utilize the `GridSearchCV` or `RandomizedSearchCV` classes from the `sklearn.model_selection` module. Here's how you can perform hyperparameter tuning using grid search with 5-fold cross-validation:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15],         # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "accuracy = best_rf_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "This code:\n",
    "- Defines a grid of hyperparameters to tune, including `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n",
    "- Creates a `GridSearchCV` object with the Random Forest Classifier, the parameter grid, 5-fold cross-validation (`cv=5`), and accuracy as the scoring metric (`scoring='accuracy'`).\n",
    "- Performs grid search using the training data.\n",
    "- Retrieves the best hyperparameters and the best model.\n",
    "- Evaluates the best model's accuracy on the test set.\n",
    "\n",
    "You can adjust the range of values for each hyperparameter in the `param_grid` dictionary based on your requirements. Similarly, you can use `RandomizedSearchCV` instead of `GridSearchCV` for randomized search. The `n_jobs` parameter specifies the number of CPU cores to use for parallelization (-1 for all available cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e676d-e994-456f-a823-f5519c2010d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performance\n",
    "metrics. Compare the performance of the tuned model with the default model.\n",
    "\n",
    "To report the best set of hyperparameters found by the hyperparameter search and the corresponding performance metrics, we can print out the best parameters and evaluate the performance of the tuned model. Additionally, we can compare the performance of the tuned model with the default model. Here's how you can do it:\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Report the best set of hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_tuned = best_rf_classifier.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "# Print performance metrics for the tuned model\n",
    "print(\"\\nPerformance Metrics for Tuned Model:\")\n",
    "print(\"Accuracy:\", accuracy_tuned)\n",
    "print(\"Precision:\", precision_tuned)\n",
    "print(\"Recall:\", recall_tuned)\n",
    "print(\"F1 Score:\", f1_tuned)\n",
    "\n",
    "# Compare with the default model\n",
    "print(\"\\nPerformance Metrics for Default Model:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "This code:\n",
    "- Prints out the best set of hyperparameters found by the grid search (`grid_search.best_params_`).\n",
    "- Evaluates the performance of the best model on the test set and computes accuracy, precision, recall, and F1 score.\n",
    "- Prints the performance metrics for the tuned model.\n",
    "- Compares the performance metrics of the tuned model with those of the default model.\n",
    "\n",
    "By comparing the performance metrics of the tuned model with those of the default model, you can assess whether the hyperparameter tuning process has improved the model's performance. If the performance metrics of the tuned model are better than those of the default model, it indicates that the hyperparameter search has successfully optimized the model's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65139542-f0ff-4b93-83e1-bb5c25d3dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\n",
    "limitations of the model for predicting heart disease risk.\n",
    "\n",
    "Interpreting the decision boundaries of a Random Forest Classifier can provide insights into how the model separates different classes in the feature space. However, due to the ensemble nature of Random Forests and the high dimensionality of the feature space, visualizing decision boundaries directly is not straightforward. One common approach is to visualize decision boundaries using a scatter plot of two of the most important features and then overlay the regions where the model predicts each class.\n",
    "\n",
    "Here's how you can plot the decision boundaries on a scatter plot of two of the most important features:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality for visualization (if necessary)\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# Train a Random Forest Classifier with the best hyperparameters\n",
    "rf_classifier_best = RandomForestClassifier(**grid_search.best_params_)\n",
    "rf_classifier_best.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundaries\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
    "y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = rf_classifier_best.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()\n",
    "\n",
    "This code:\n",
    "- Reduces the dimensionality of the feature space to 2 dimensions using Principal Component Analysis (PCA) for visualization purposes.\n",
    "- Trains a Random Forest Classifier with the best hyperparameters found by the grid search.\n",
    "- Plots the decision boundaries of the trained classifier on a scatter plot of the two most important features.\n",
    "- Overlay data points on the plot, where each point represents an instance in the dataset, colored by its class label.\n",
    "\n",
    "Interpreting the decision boundaries and insights of the model:\n",
    "- Decision boundaries separate different classes in the feature space. Regions where the same class is predicted by the model are enclosed by decision boundaries.\n",
    "- The scatter plot of the two most important features provides a visualization of how the model separates instances belonging to different classes.\n",
    "- Insights from the decision boundaries can help understand how the model makes predictions and identify regions where the model might struggle to generalize.\n",
    "- Limitations include the inability to visualize high-dimensional feature spaces directly and the potential for overfitting in regions of high data density.\n",
    "\n",
    "Interpreting decision boundaries should be done cautiously, especially in high-dimensional spaces, as visualizations may not capture the full complexity of the model's behavior. Additionally, other evaluation metrics and techniques, such as feature importance analysis and model interpretation methods, can provide complementary insights into the model's performance and behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
